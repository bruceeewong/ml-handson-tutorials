# Dom2Vec Application Classifier - Technical Design Analysis

**Document Version**: 1.0  
**Date**: January 11, 2025  
**Author**: Technical Analysis Team  
**Status**: Initial Architecture Review  

## Executive Summary

This document provides a comprehensive technical analysis of the Dom2Vec Application Classifier system, which adapts the Dom2Vec approach from DGA detection to application classification from SNI domain names. The system achieves a proof-of-concept implementation but shows significant performance limitations (37.9% accuracy) that require architectural improvements.

## Table of Contents

1. [System Architecture](#system-architecture)
2. [Component Analysis](#component-analysis)
3. [Training Pipeline](#training-pipeline)
4. [Performance Analysis](#performance-analysis)
5. [Design Strengths](#design-strengths)
6. [Critical Limitations](#critical-limitations)
7. [Improvement Recommendations](#improvement-recommendations)
8. [Version History](#version-history)

## System Architecture

### High-Level Design

The system implements a **two-stage machine learning pipeline** that combines semantic understanding with interpretable classification:

```
Raw Domain ‚Üí Domain Processing ‚Üí Word2Vec Embeddings ‚Üí Feature Engineering ‚Üí Decision Tree ‚Üí Application Prediction
```

### Core Components

| Component | File | Purpose | Input | Output |
|-----------|------|---------|-------|--------|
| Domain Processor | `domain_processor.py` | Tokenization & Feature Extraction | Raw domain string | Tokens + structural features |
| Word2Vec Model | `dom2vec_app_classifier.py` | Semantic embeddings | Token sequences | 100D vectors |
| Feature Engineering | `dom2vec_app_classifier.py` | Combine embeddings + features | Embeddings + domain features | 107D feature vector |
| Decision Tree | `dom2vec_app_classifier.py` | Classification | Feature vectors | Application labels |

### Data Flow Architecture

```mermaid
graph TB
    A[Raw Domain<br/>api.spotify.com] --> B[Domain Processor]
    B --> C[Tokens<br/>['api', 'spotify', 'com']]
    C --> D[Word2Vec<br/>Embedding]
    C --> E[Structural<br/>Features]
    D --> F[100D Vector<br/>Semantic representation]
    E --> G[7D Vector<br/>Domain characteristics]
    F --> H[Combined 107D<br/>Feature Vector]
    G --> H
    H --> I[Decision Tree<br/>Classifier]
    I --> J[Prediction<br/>spotify (0.945 conf)]
```

## Component Analysis

### 1. Domain Processor (`domain_processor.py`)

**Capabilities**:
- ‚úÖ **Pattern-based tokenization**: Handles `user123` ‚Üí `user123` (not `['user', '1', '2', '3']`)
- ‚úÖ **TLD recognition**: Preserves `.com`, `.net` as single tokens
- ‚úÖ **DGA detection**: Identifies suspicious vs legitimate domains
- ‚úÖ **Smart number handling**: Years (`2024`) and IDs preserved as units

**Implementation Quality**: **High** - Well-designed with regex patterns and heuristics

**Example Tokenization**:
```python
"api.spotify.com"           ‚Üí ['api', 'spotify', 'com']
"teams.microsoft.com"       ‚Üí ['teams', 'microsoft', 'com']  
"user123.github.io"         ‚Üí ['user123', 'github', 'io']
"my-app-2024.herokuapp.com" ‚Üí ['my', 'app', '2024', 'herokuapp', 'com']
```

### 2. Word2Vec Embeddings

**Configuration**:
- **Vector Size**: 100 dimensions
- **Window**: 5-token context
- **Algorithm**: CBOW (Continuous Bag of Words)
- **Vocabulary**: 161 unique tokens from 144 training domains

**Semantic Relationships Learned**:
- `'spotify'` ‚âà `'music'` (conceptually related)
- `'api'` ‚âà `'rest'` (technical similarity)
- `'google'` ‚âà `'youtube'` (company relationship)

**Pooling Strategy**: Simple averaging across domain tokens

### 3. Feature Engineering

**Combined Feature Vector (107 dimensions)**:

```python
# Embedding Features (100D)
embedding_features = average(word2vec_vectors_for_tokens)

# Structural Features (7D)  
structural_features = [
    domain_length,        # Character count
    subdomain_count,      # Number of dots
    has_numbers,          # Boolean: contains digits
    has_hyphens,          # Boolean: contains hyphens  
    has_underscores,      # Boolean: contains underscores
    token_count,          # Number of meaningful tokens
    avg_token_length      # Average token character length
]

final_features = concat(embedding_features, structural_features)
```

### 4. Decision Tree Classifier

**Parameters**:
- `max_depth=15` (moderately deep)
- `min_samples_split=5` (conservative)
- `min_samples_leaf=2` (small leaves)
- `random_state=42` (reproducible)

**Interpretability**: Provides clear decision paths showing feature importance

## Training Pipeline

### Training Data Structure

**Dataset Composition**:
- **Total samples**: 144 domains
- **Applications**: 11 classes
- **Distribution**: 10-18 domains per application (relatively balanced)

```python
APPLICATION_DOMAINS = {
    'netflix': 13 domains,           # netflix.com, nflxvideo.net, ...
    'spotify': 13 domains,           # spotify.com, scdn.co, ...
    'youtube': 13 domains,           # youtube.com, googlevideo.com, ...
    'microsoft_teams': 12 domains,   # teams.microsoft.com, ...
    'zoom': 13 domains,              # zoom.us, zmcdn.com, ...
    'google_services': 18 domains,   # google.com, googleapis.com, ...
    'facebook_meta': 15 domains,     # facebook.com, fbcdn.net, ...
    'amazon_aws': 14 domains,        # amazon.com, amazonaws.com, ...
    'apple_services': 13 domains,    # apple.com, icloud.com, ...
    'github': 10 domains,            # github.com, githubusercontent.com, ...
    'slack': 10 domains              # slack.com, slack-edge.com, ...
}
```

### Training Steps

1. **Domain Tokenization** (144 domains ‚Üí 144 token sequences)
2. **Word2Vec Training** (Create 161-word vocabulary with embeddings)
3. **Feature Extraction** (Generate 107D vectors for each domain)
4. **Train/Test Split** (80%/20% stratified split)
5. **Decision Tree Training** (Fit classifier on training features)
6. **Evaluation** (Test accuracy + 5-fold cross-validation)

## Performance Analysis

### Current Performance Metrics

| Metric | Value | Status |
|--------|-------|--------|
| Test Accuracy | 37.9% | ‚ùå Poor |
| CV Accuracy | 33.0% ¬± 9.0% | ‚ùå Poor + High variance |
| Training Time | 0.09 seconds | ‚úÖ Fast |
| Vocabulary Size | 161 tokens | ‚ö†Ô∏è Small for embeddings |
| Model Size | ~1-5MB | ‚úÖ Compact |

### Per-Class Performance Issues

From classification report:
- **Good performers**: `amazon_aws` (67% recall), `facebook_meta` (33% recall)
- **Poor performers**: `apple_services`, `microsoft_teams`, `netflix`, `slack` (0% recall)
- **High precision, low recall**: Most classes - model is conservative but misses many samples

### Mathematical Analysis of Performance Issues

**Dimensionality Problem**:
```
Dataset size: 144 samples
Feature dimensions: 107
Classes: 11
Samples per class: 144/11 ‚âà 13
Feature-to-sample ratio: 107/13 ‚âà 8.2

Rule of thumb violation: Need 10+ samples per feature per class
Current ratio: 0.12 samples per feature per class
```

**Word2Vec Training Issues**:
- **Vocabulary size**: 161 words from only 144 domains
- **Token frequency**: Many tokens appear only 1-2 times
- **Embedding quality**: Poor representations for rare tokens

## Design Strengths

### ‚úÖ Architectural Decisions

1. **Two-stage approach** (Embeddings + Traditional ML)
   - **Benefit**: Combines semantic understanding with interpretability
   - **Use case**: Can explain why domain classified as specific app

2. **Word2Vec over modern transformers**
   - **Benefit**: Faster training, lower complexity, sufficient for domain tokens
   - **Trade-off**: Less sophisticated but more efficient

3. **Decision trees over neural networks**
   - **Benefit**: Interpretable decisions, handles small data better
   - **Use case**: Can show decision path: "If embed_42 ‚â§ 0.23 ‚Üí Spotify"

4. **Pattern-based domain processing**
   - **Benefit**: Handles edge cases better than simple regex
   - **Example**: `user123` stays as one token, not split into `['user', '1', '2', '3']`

### ‚úÖ Implementation Quality

- **Modular design**: Clear separation of concerns
- **Extensible**: Easy to add new applications
- **Configurable**: Hyperparameters exposed via command line
- **Well-documented**: Clear docstrings and examples

## Critical Limitations

### ‚ùå Performance Issues

1. **Low Accuracy (37.9%)**
   - **Root cause**: Insufficient training data for complexity
   - **Impact**: Not suitable for production use

2. **Poor Generalization**
   - **Evidence**: CV score (33%) < Test score (37.9%)
   - **Cause**: Model overfitting to small dataset

### ‚ùå Data Issues

3. **Curse of Dimensionality**
   - **Problem**: 107 features vs 144 samples
   - **Effect**: Model cannot learn meaningful patterns
   - **Solution needed**: Feature selection or more data

4. **Insufficient Word2Vec Training**
   - **Issue**: 161 vocabulary from 144 domains
   - **Result**: Poor embeddings for rare tokens
   - **Evidence**: Many tokens appear only once

### ‚ùå Design Issues

5. **Feature Imbalance**
   - **Problem**: Embeddings (100D) dominate structural features (7D)
   - **Effect**: Domain-specific insights lost
   - **Ratio**: 93% embedding features vs 7% structural

6. **Simple Pooling Strategy**
   - **Current**: Naive averaging of Word2Vec vectors
   - **Missing**: Attention weights, position importance, token significance

7. **No Hierarchical Structure**
   - **Issue**: Treats all 11 classes as equally distinct
   - **Reality**: Some apps more similar (Google services vs YouTube)
   - **Opportunity**: Group similar applications first

## Improvement Recommendations

### üöÄ Priority 1: Data and Feature Engineering

1. **Data Augmentation**
   ```python
   # Generate synthetic domains
   api-{service}.com, cdn-{service}.net, {service}-static.com
   # Target: 500+ domains per application
   ```

2. **Feature Selection**
   ```python
   # Reduce from 107 to top 20-30 features
   # Use feature importance scores from current model
   # Focus on most discriminative embedding dimensions
   ```

3. **Better Embedding Pooling**
   ```python
   # Weighted average based on token importance
   weights = attention_mechanism(tokens)
   embedding = weighted_average(embeddings, weights)
   ```

### üöÄ Priority 2: Model Architecture

4. **Ensemble Methods**
   ```python
   # Replace single decision tree with Random Forest
   RandomForestClassifier(n_estimators=100, max_depth=10)
   # Better handling of high-dimensional data
   ```

5. **Hierarchical Classification**
   ```python
   # Stage 1: Group similar apps
   tech_companies = ['google_services', 'youtube', 'microsoft_teams']
   media_streaming = ['netflix', 'spotify', 'youtube']
   
   # Stage 2: Classify within groups
   ```

### üöÄ Priority 3: Feature Engineering

6. **Domain-Specific Features**
   ```python
   additional_features = [
       'has_api_keyword',     # api, rest, graphql
       'has_cdn_pattern',     # cdn, static, assets
       'is_subdomain',        # multiple dots
       'tld_type',            # com, net, org, io
       'company_indicators'   # known company tokens
   ]
   ```

7. **Better Text Processing**
   ```python
   # Use TF-IDF for token importance
   # N-gram features for token sequences
   # Character-level features for unknown tokens
   ```

### üöÄ Priority 4: Validation and Testing

8. **Cross-Domain Validation**
   - Test on completely different domains not in training
   - Validate on real network traffic data
   - A/B test against baseline methods

9. **Confidence Calibration**
   - Implement proper confidence scoring
   - Set confidence thresholds for reliable predictions
   - Add "unknown" class for low-confidence predictions

## Implementation Priority Matrix

| Improvement | Impact | Effort | Priority |
|-------------|--------|--------|----------|
| Feature Selection | High | Low | üî• Immediate |
| Data Augmentation | High | Medium | üî• Immediate |
| Random Forest | Medium | Low | üî• Immediate |
| Better Pooling | Medium | Medium | ‚≠ê Short-term |
| Hierarchical Classification | High | High | ‚≠ê Short-term |
| Domain-Specific Features | Medium | Medium | ‚≠ê Short-term |
| Cross-Domain Validation | High | High | üìÖ Long-term |

## Version History

### Version 1.0 (January 11, 2025)
- **Status**: Initial architecture review and performance analysis
- **Scope**: Complete system design analysis
- **Key Findings**: 
  - Proof-of-concept working but low performance (37.9% accuracy)
  - Identified dimensionality and data insufficiency issues
  - Provided detailed improvement roadmap
- **Next Steps**: Implement Priority 1 improvements (feature selection, data augmentation)

---

**Document Maintainers**: Technical Analysis Team  
**Review Cycle**: Quarterly or after major architecture changes  
**Related Documents**: README.md, requirements.txt, training logs  
**Code Repository**: `/decision_tree/` directory 